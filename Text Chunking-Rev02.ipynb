{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Chunking for Noun Phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'These are remarks about text Chunking of Noun Phrases'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(__doc__)  # Attribute prints '''remarks'''\n",
    "'''These are remarks about text Chunking of Noun Phrases'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Needed for Noun Phrase work-around\n",
    "#from io import StringIO  # from StringIO import cStringIO  #Python2\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk.chunk\n",
    "from nltk.tag import pos_tag\n",
    "#from nltk.draw.tree import TreeSegmentWidget, tree_to_treesegment\n",
    "\n",
    "import itertools\n",
    "import collections\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 399276\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    This property OUTSTANDING offers outdoor livin...\n",
       "1      Enjoy your private resort in your own backyard.\n",
       "2                           Indoors is also remodeled.\n",
       "3                        Kitchen opens to Family Room.\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read 'PublicRemarks' Text from file (00m:31s)\n",
    "textfile = open('PublicRemarksCorpus.txt', 'r') # File\n",
    "text = textfile.read()  # String\n",
    "#text = text[0:493]\n",
    "\n",
    "## Convert Corpus to list of sentences (00m:26s)\n",
    "df = pd.Series(sent_tokenize(text))\n",
    "print('Number of sentences:',len(df))\n",
    "#df = df[0:400]\n",
    "df[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 399276\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    This property OUTSTANDING offers outdoor livin...\n",
       "1      Enjoy your private resort in your own backyard.\n",
       "2                           Indoors is also remodeled.\n",
       "3                        Kitchen opens to Family Room.\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Convert Corpus to list of sentences (00m:26s)\n",
    "df = pd.Series(sent_tokenize(text))\n",
    "print('Number of sentences:',len(df))\n",
    "#df = df[0:400]\n",
    "df[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df has length: 399276\n",
      "\n",
      "0    This property OUTSTANDING offers outdoor livin...\n",
      "1       Enjoy your private resort in your own backyard\n",
      "2                            Indoors is also remodeled\n",
      "3                         Kitchen opens to Family Room\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "## Clean Text (00m:07s)\n",
    "df = df.apply(lambda x : re.sub(r'[\\b\\(\\)\\\\\\\"\\'\\/\\[\\]\\s+\\,\\.:\\?!;#*=+]', ' ', x))  # '-' is under debate\n",
    "df = df.apply(lambda x : re.sub( '\\s+', ' ', x).strip() )  # remove white space\n",
    "\n",
    "# Test\n",
    "print('df has length: %i\\n' % len(df))\n",
    "print(df[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Gather noun phrase (NP) from sentence tree (01h:24m:--s)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "    # Define several tag patterns\n",
    "grammar = r\"\"\"\n",
    "      NP: {<DT|PP\\$>?<JJ>*<NN>}   # chunk determiner/possessive, adjectives and noun\n",
    "          {<NNP>+}                # chunk sequences of proper nouns\n",
    "          {<NN>+}                 # chunk consecutive nouns\n",
    "          {<CD>+}                 # chunk cardinal numbers i.e. Zip Codes\n",
    "          {<JJ>+}                 # chunk hyphenated terms\n",
    "          \"\"\"\n",
    "def extract_np(parsed_sent):\n",
    "      for subtree in parsed_sent.subtrees():\n",
    "        if subtree.label() == 'NP':\n",
    "          yield ' '.join(word for word, tag in subtree.leaves())\n",
    "\n",
    "def nounPhrase(sentence, grammar):\n",
    "    x=[]  # declare x as list\n",
    "    if len(sentence) < 1: \n",
    "        pass\n",
    "    else:\n",
    "        tagged_sent = pos_tag(sentence.split())  # List of tuples with [(Word, PartOfSpeech)]\n",
    "        cp = nltk.RegexpParser(grammar)\n",
    "        parsed_sent = cp.parse(tagged_sent)\n",
    "        for npstr in extract_np(parsed_sent):\n",
    "            x.append(npstr)  #print(npstr)\n",
    "    return x\n",
    "\n",
    "df2 = df.apply(lambda x : nounPhrase(x, grammar))\n",
    "\n",
    "stop_time = time.time()\n",
    "\n",
    "print(\"Runtime: %.2fs\" % (stop_time - start_time))\n",
    "os.system(\"say 'Computation Complete!'\")\n",
    "\n",
    "df2[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## All Noun Phrases\n",
    "NounPhrases = list(np.concatenate(df2))  # chain = np.concatenate(df2), then convert type to list\n",
    "\n",
    "# Test\n",
    "#print(NounPhrases[0:10])\n",
    "#print(len(NounPhrases)) # 2.0 M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NounPhrases - Save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Save NounPhrases to JSON\n",
    "## ====> DANGER <==== Don't overwrite your NounPhrases!!!\n",
    "#with open('NounPhrases.json', 'w') as outfile:\n",
    "#    json.dump(NounPhrases, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Read Dict from file\n",
    "f = open('NounPhrases.json','r')\n",
    "data = f.read()\n",
    "NPs = json.loads(data)\n",
    "\n",
    "##Test\n",
    "print(NPs[0:10])\n",
    "print(len(NPs))\n",
    "print(type(NPs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion to n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## NounPhraseList (order & count are lost! Sorted instead.)\n",
    "NounPhraseList = sorted(list(set(map(lambda x:x.lower(),NPs))))  ## Create 'NPList', a set of NounPhrases of type list\n",
    "\n",
    "#Test\n",
    "print(NounPhraseList[17000:17020])\n",
    "print(len(NounPhraseList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Save n-grams to JSON\n",
    "## ====> DANGER <==== Don't overwrite your ngrams!!!\n",
    "#with open('ngrams.json', 'w') as outfile:\n",
    "#    json.dump(ngrams, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create Counter\n",
    "# Ex. a = [1,1,1,1,2,2,2,2,3,3,4,5,5]\n",
    "#counter = collections.Counter(dictionary)  # Specialized Dictionary\n",
    "#print(counter)\n",
    "# Ex. Counter({1: 4, 2: 4, 3: 2, 5: 2, 4: 1})\n",
    "#print(counter.values())\n",
    "# Ex. [4, 4, 2, 1, 2]\n",
    "#print(counter.keys())\n",
    "# Ex. [1, 2, 3, 4, 5]\n",
    "#print(counter.most_common(500))\n",
    "\n",
    "#grams = counter.most_common(20)  # top unigrams\n",
    "#grams[0:10]\n",
    "\n",
    "## Thoughts\n",
    "# probably worth comparing to the nltk n-gram tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Example of Boolean Variance calculation\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "#X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\n",
    "P_True = 0.8\n",
    "sel = VarianceThreshold(threshold=(P_True * (1 - P_True)))  # Boolean Var() clac\n",
    "sel.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Univariate Feature Selection\n",
    "## Source: http://scikit-learn.org/stable/modules/feature_selection.html\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "print(X.shape)\n",
    "X_new = SelectKBest(chi2, k=2).fit_transform(X, y)\n",
    "print(X_new.shape)\n",
    "print(type(X_new))\n",
    "print(X[0:10])\n",
    "print(X_new[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read Dictionary from file\n",
    "f = open('dictionary.json','r')\n",
    "data = f.read()\n",
    "dictionary = json.loads(data)\n",
    "dictionary[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## New approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read Dictionary from file\n",
    "f = open('PublicRemarksCorpus.txt','r')\n",
    "corpus = f.read()\n",
    "corpus[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_candidate_words(text, good_tags=set(['JJ','JJR','JJS','NN','NNP','NNS','NNPS'])):\n",
    "    import itertools, nltk, string\n",
    "\n",
    "    # exclude candidates that are stop words or entirely punctuation\n",
    "    punct = set(string.punctuation)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    # tokenize and POS-tag words\n",
    "    tagged_words = itertools.chain.from_iterable(nltk.pos_tag_sents(nltk.word_tokenize(sent)\n",
    "                                                                    for sent in nltk.sent_tokenize(text)))\n",
    "    # filter on certain POS tags and lowercase all words\n",
    "    candidates = [word.lower() for word, tag in tagged_words\n",
    "                  if tag in good_tags and word.lower() not in stop_words\n",
    "                  and not all(char in punct for char in word)]\n",
    "\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candidates = extract_candidate_words(corpus)\n",
    "candidates[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_keyphrases_by_tfidf(texts, candidates='chunks'):\n",
    "    import gensim, nltk\n",
    "    \n",
    "    # extract candidates from each text in texts, either chunks or words\n",
    "    if candidates == 'chunks':\n",
    "        boc_texts = [extract_candidate_chunks(text) for text in texts]\n",
    "    elif candidates == 'words':\n",
    "        boc_texts = [extract_candidate_words(text) for text in texts]\n",
    "    # make gensim dictionary and corpus\n",
    "    dictionary = gensim.corpora.Dictionary(boc_texts)\n",
    "    corpus = [dictionary.doc2bow(boc_text) for boc_text in boc_texts]\n",
    "    # transform corpus with tf*idf model\n",
    "    tfidf = gensim.models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    \n",
    "    return corpus_tfidf, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_keyphrases_by_tfidf(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun Phrase Importance using TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read Dictionary from file\n",
    "f = open('NounPhrases.json','r')\n",
    "NounPhrases = json.load(f)\n",
    "\n",
    "# LowerCase\n",
    "NounPhrases = [item.lower() for item in NounPhrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "#from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    " \n",
    "def textrank(NounPhrases):\n",
    "    #sentence_tokenizer = PunktSentenceTokenizer()\n",
    "    #sentences = sentence_tokenizer.tokenize(document)\n",
    " \n",
    "    bow_matrix = CountVectorizer().fit_transform(NounPhrases)\n",
    "    normalized = TfidfTransformer().fit_transform(bow_matrix)\n",
    " \n",
    "    similarity_graph = normalized * normalized.T\n",
    " \n",
    "    nx_graph = nx.from_scipy_sparse_matrix(similarity_graph)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    return sorted(((scores[i],s) for i,s in enumerate(NounPhrases)),\n",
    "                  reverse=True)  # Output ordered rank of NounPhrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ranked = textrank(NounPhrases[0:10000])  # [0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0002575679147337191,\n",
       " 'bath in great gilbert location all appliances are included tile in kitchen and bathrooms ceiling fans rv gate')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked[50]  # [0:5][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this property',\n",
       " 'outstanding',\n",
       " 'outdoor',\n",
       " 'living',\n",
       " 'entertainment',\n",
       " '85254',\n",
       " 'enjoy',\n",
       " 'private resort',\n",
       " 'own backyard']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NounPhrases[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Document Summarization Using TextRank\n",
    "\n",
    "This will identify the most relevant sentences.  \n",
    "Source: https://joshbohde.com/blog/document-summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This property OUTSTANDING offers outdoor living spaces, enabling you to have unrivaled entertainment in the famed 85254.  Enjoy your private resort in your own backyard. Indoors is also remodeled.  Kitchen opens to Family Room. Large glass doors lead to backyard oasis. Pool & outdoor living room/ramada are some of the many fantastic qualities. Spacious 4 bedrooms + spacious flex room or office.  5 Year Roof Warranty, some new windows too!  Already great home with unlimited possibilities. With 8/10 of a lush acre and lots of room to roam or add on. Amazing opportunity to own in the coveted 85254 zip code. PERFECT LOCATION. 10 minutes from 7 golf courses and 12 minutes from Sky Harbor airport.  See for yourself...you will LOVE Hurry before It's gone! Very special home in popular Arabian Views feels like you are on vacation!  Open, airy and neutral decor.  Travertine stone flooring. Split floor plan, 4 beds, 2 baths with updating in all the right places! Granite counters in kitchen and baths. Gorgeous tiled showers.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Dictionary from file\n",
    "f = open('PublicRemarksCorpus.txt','r')\n",
    "corpus = f.read()\n",
    "corpus[0:1028]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    " \n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    " \n",
    "def textrank(document):\n",
    "    sentence_tokenizer = PunktSentenceTokenizer()\n",
    "    sentences = sentence_tokenizer.tokenize(document)\n",
    " \n",
    "    bow_matrix = CountVectorizer().fit_transform(sentences)\n",
    "    normalized = TfidfTransformer().fit_transform(bow_matrix)\n",
    " \n",
    "    similarity_graph = normalized * normalized.T\n",
    " \n",
    "    nx_graph = nx.from_scipy_sparse_matrix(similarity_graph)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    return sorted(((scores[i],s) for i,s in enumerate(sentences)),\n",
    "                  reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ranked = textrank(corpus[0:102800])  # Up to 10k, before slow-down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gorgeous single level home in Pinnacle Reserve.Enter thru the front door to a large open great room with lots of windows and natural light and views of the patio and pebbletec pool.There is a formal dining area off the great room that can also be used as an office space.Large eat-in kitchen with tons of cabinet and counter space overlooks family room.Split floorplan with a huge bonus room/play room/mother-in-law suite.Private master bedroom with lots of natural light,double sink vanity, separate shower/tub and walk-in closet.2nd and 3rd bedrooms are good size with accompanying full bath with double sinks.Interior of the house was recently repainted as well as the garage.Covered patio overlooks sparkling pool with South facing backyard with plenty of sunshine.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"North-South exposure.Buyer verify all info ***BEST PRICE ON SINGLE-FAMILY HOME IN TROON*** HOUSE SITS ON CHOICE LOT ON CUL-DE-SAC STREET * ENJOY THE AMAZING MOUNTAIN & DESERT VIEWS AS YOU RELAX IN ARIZONA-STYLE ON YOUR LARGE COVERED PATIO WITH BEAUTIFUL EXTERIOR LIGHTING & HEATED POOL & SPA * GREAT ATMOSPHERE FOR ENTERTAINING * SPACIOUS, FLOWING FLOOR PLAN * SPLIT MASTER BEDROOM WITH SEPARATE EXIT TO BACKYARD * MASTER BATH FEATURES DUAL-SINKS, SOAKING TUB & SEPARATE SHOWER PLUS WALK-IN CLOSET * 2 ADD'L BEDS & BATHS * FORMAL DINING & LIVING AREAS PLUS FAMILY ROOM WITH COZY GAS FIREPLACE * ISLAND-KITCHEN IS OPEN AND HAS LOTS OF CABINETS & COUNTER SPACE & BUILT-IN DESK * 2 & 1/2 CAR GARAGE WITH BUILT-IN CABINETS & EXTRA STORAGE SPACE * A GREAT HOME FOR FULL OR PART-TIME RESIDENTS * CLOSE TO GOLF, SHOPPING & RESTAURANTS!\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Junk!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(type(SentenceTree))\n",
    "print(SentenceTree)\n",
    "print(NounPhrases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for subtree in chunked_sent.subtrees():\n",
    "    if subtree.label() == 'NP': print(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SentenceTree.draw()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def traverseTree(tree):\n",
    "    #print(\"tree:\", tree)\n",
    "    set = []\n",
    "    for subtree in tree:\n",
    "        if type(subtree) == nltk.tree.Tree:\n",
    "            traverseTree(subtree)\n",
    "        else:\n",
    "            if tree.label() == 'NP':\n",
    "                set += tree[0][0]  #print(tree[0][0])\n",
    "    return set\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "SentenceTree = cp.parse(tagged_sent)\n",
    "#print(SentenceTree)\n",
    "\n",
    "NounPhrases2 = traverseTree(SentenceTree)\n",
    "print(NounPhrases2)\n",
    "\n",
    "#SentenceTree.leaves()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#import urllib\n",
    "#from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "#from nltk.tokenize import word_tokenize\n",
    "#from nltk.tokenize import RegexpTokenizer\n",
    "#from nltk.collocations import *\n",
    "#import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Python code for segmentation, POS tagging and tokenization\n",
    "rawtext = open('PublicRemarksCorpus.txt').read()\n",
    "sentences = nltk.sent_tokenize(rawtext) # NLTK default sentence segmenter \n",
    "sentences = [nltk.word_tokenize(sent) for sent in sentences] # NLTK word tokenizer\n",
    "sentences = [nltk.pos_tag(sent) for sent in sentences] # NLTK POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Patterns = \"\"\"\n",
    "        NP:     {<DT|PP\\$>?<JJ>*<NN>}\n",
    "                {<NNP>+}\n",
    "                {<NN>+}\n",
    "\"\"\"\n",
    "\n",
    "NPChunker = nltk.RegexpParser(patterns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a corpus and conduct analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Concatenate 'PublicRemarks' into a corpus\n",
    "def concatCol(col):\n",
    "    text = ''  # Declare text as string\n",
    "    for n in range(len(col)): # \n",
    "        text += str(col[n])+' '\n",
    "    return text\n",
    "\n",
    "corpus = concatCol(df['PublicRemarks'])\n",
    "\n",
    "## Save Text to File\n",
    "file = open('PublicRemarksCorpus.txt', \"w\")\n",
    "file.write(corpus)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read 'PublicRemarks' Text from file\n",
    "textfile = open('PublicRemarksCorpus.txt', 'r') # File\n",
    "corpus = textfile.read()  # String\n",
    "#corpus[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def get_ngrams(text, n ):\n",
    "    text = re.sub(r'[\\b\\(\\)\\\\\\\"\\'\\/\\[\\]\\s+\\,\\.:\\?;]', ' ', text)\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    n_grams = ngrams(tokens, n)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    "\n",
    "unigrams = get_ngrams(corpus, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
