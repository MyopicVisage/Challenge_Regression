{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Chunking for Noun Phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Needed for Noun Phrase work-around\n",
    "from io import StringIO  # from StringIO import cStringIO  #Python2\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk.chunk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.draw.tree import TreeSegmentWidget, tree_to_treesegment\n",
    "\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Read 'PublicRemarks' Text from file\n",
    "textfile = open('PublicRemarksCorpus.txt', 'r') # File\n",
    "text = textfile.read()  # String\n",
    "#text = text[0:493]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Convert Corpus to list of sentences\n",
    "df = pd.Series(sent_tokenize(text))\n",
    "#print(len(sent_tokenize_list))\n",
    "#sent_tokenize_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Clean Text - Runs slow and isn't parallelized, try pypi and/or multiprocessing\n",
    "df = df.apply(lambda x : re.sub(r'[\\b\\(\\)\\\\\\\"\\'\\/\\[\\]\\s+\\,\\.:\\?!;]', ' ', x))\n",
    "df\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Gather noun phrase (NP) from sentence tree\n",
    "class Capturing(list):\n",
    "    def __enter__(self):\n",
    "        self._stdout = sys.stdout\n",
    "        sys.stdout = self._stringio = StringIO()\n",
    "        return self\n",
    "    def __exit__(self, *args):\n",
    "        self.extend(self._stringio.getvalue().splitlines())\n",
    "        sys.stdout = self._stdout\n",
    "\n",
    "def traverse(t):\n",
    "    try:\n",
    "        t.label()\n",
    "    except AttributeError:\n",
    "        return\n",
    "    else:\n",
    "        if t.label() == 'NP': print(t)  # or do something else\n",
    "        else:\n",
    "            for child in t: \n",
    "                traverse(child)\n",
    "\n",
    "def nounPhrase(sentence):\n",
    "    # Tag sentence for part of speech\n",
    "    tagged_sent = pos_tag(sentence.split())  # List of tuples with [(Word, PartOfSpeech)]\n",
    "    # Define several tag patterns\n",
    "    grammar = r\"\"\"\n",
    "      NP: {<DT|PP\\$>?<JJ>*<NN>}   # chunk determiner/possessive, adjectives and noun\n",
    "          {<NNP>+}                # chunk sequences of proper nouns\n",
    "          {<NN>+}                 # chunk consecutive nouns\n",
    "          {<CD>+}                 # chunk cardinal numbers i.e. Zip Codes\n",
    "          {<JJ>+}                 # chunk hyphenated terms\n",
    "          \"\"\"\n",
    "\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    SentenceTree = cp.parse(tagged_sent)\n",
    "    #NounPhrases = traverse(SentenceTree)\n",
    "\n",
    "    with Capturing() as output:  # work-around for recursive traverse statement\n",
    "        traverse(SentenceTree)   # returns a variable 'output'\n",
    "    \n",
    "    #NounPhrases = output\n",
    "    #print(NounPhrases)\n",
    "    return(output)\n",
    "\n",
    "df2 = df.apply(lambda x : nounPhrase(x))\n",
    "df2 = list(df2)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Clean up Noun Phrases \n",
    "## Maybe later...\n",
    "#df2 = df2.apply(lambda x : x.str.replace('(NP ', '') )\n",
    "#df2 = df2.replace('/NN', '')\n",
    "\n",
    "print(df2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## dictionary\n",
    "chain = np.concatenate(df2)\n",
    "#chain = itertools.chain.from_iterable(df2)\n",
    "dictionary = list(chain)\n",
    "#print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(dictionary)) # 2.4 M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Remove Duplicates (order & count are lost!)\n",
    "dictionary = set(dictionary)\n",
    "' #1/NNP' in dictionary\n",
    "#dict = list(dictionary)\n",
    "#len(dict)  # 209 k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Save Dictionary to JSON\n",
    "with open('dict.json', 'w') as outfile:\n",
    "    json.dump(dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read Dictionary from file\n",
    "f = open('dict.json','r')\n",
    "data = f.read()\n",
    "dict = json.loads(data)\n",
    "dict[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sorted(Dictionary)[0:100]) # print first strings in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Replace Text to clean list\n",
    "dict = set(Dictionary)  # Use to create the Attribute set\n",
    "print(sorted(dict)[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'(NP lease/NN)' in dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Unigram, Bigram, Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Deprecated\n",
    "## Test Sentences for Debugging\n",
    "#text = 'Have you been to the fixer-upper in the 84505 6208 area code?'\n",
    "#text = 'We saw the yellow dog.'\n",
    "text = 'Robert Johnson, the cat in the hat, shot his giant cannon!!!' #Why did he do that?'\n",
    "#text = \"Michael Jackson likes to eat at McDonalds.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Deprecated!\n",
    "## Clean Text\n",
    "# Remove punctuation and set to lower case\n",
    "text = re.sub(r'[\\b\\(\\)\\\\\\\"\\'\\/\\[\\]\\s+\\,\\.:\\?!;]', ' ', text)\n",
    "# break into lines and remove leading and trailing space on each\n",
    "lines = (line.strip() for line in text.splitlines())\n",
    "# break multi-headlines into a line each\n",
    "chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "# drop blank lines\n",
    "sentence = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Deprecated!\n",
    "sentence\n",
    "pos_tag(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Deprecated\n",
    "# Tag sentence for part of speech\n",
    "tagged_sent = pos_tag(sentence.split())  # List of tuples with [(Word, PartOfSpeech)]\n",
    "# Define several tag patterns\n",
    "grammar = r\"\"\"\n",
    "      NP: {<DT|PP\\$>?<JJ>*<NN>}   # chunk determiner/possessive, adjectives and noun\n",
    "          {<NNP>+}                # chunk sequences of proper nouns\n",
    "          {<NN>+}                 # chunk consecutive nouns\n",
    "          {<CD>+}                 # chunk cardinal numbers i.e. Zip Codes\n",
    "          {<JJ>+}                 # chunk hyphenated terms\n",
    "          \"\"\"\n",
    "\n",
    "#cp = nltk.RegexpParser(grammar)\n",
    "#SentenceTree = cp.parse(tagged_sent)\n",
    "    #NounPhrases = traverse(SentenceTree)\n",
    "    #print('')\n",
    "    #print(NounPhrases)\n",
    "\n",
    "\n",
    "\n",
    "#with Capturing() as NounPhrases:  # work-around for recursive traverse statement\n",
    "cp = nltk.RegexpParser(grammar)       # configure parser\n",
    "chunked_sent = cp.parse(tagged_sent)  # Tree Object\n",
    "traverse(chunked_sent)  #  NounPhrases = \n",
    "    \n",
    "#print(NounPhrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(type(SentenceTree))\n",
    "print(SentenceTree)\n",
    "print(NounPhrases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for subtree in chunked_sent.subtrees():\n",
    "    if subtree.label() == 'NP': print(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SentenceTree.draw()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def traverseTree(tree):\n",
    "    #print(\"tree:\", tree)\n",
    "    set = []\n",
    "    for subtree in tree:\n",
    "        if type(subtree) == nltk.tree.Tree:\n",
    "            traverseTree(subtree)\n",
    "        else:\n",
    "            if tree.label() == 'NP':\n",
    "                set += tree[0][0]  #print(tree[0][0])\n",
    "    return set\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "SentenceTree = cp.parse(tagged_sent)\n",
    "#print(SentenceTree)\n",
    "\n",
    "NounPhrases2 = traverseTree(SentenceTree)\n",
    "print(NounPhrases2)\n",
    "\n",
    "#SentenceTree.leaves()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#import urllib\n",
    "#from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "#from nltk.tokenize import word_tokenize\n",
    "#from nltk.tokenize import RegexpTokenizer\n",
    "#from nltk.collocations import *\n",
    "#import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Python code for segmentation, POS tagging and tokenization\n",
    "rawtext = open('PublicRemarksCorpus.txt').read()\n",
    "sentences = nltk.sent_tokenize(rawtext) # NLTK default sentence segmenter \n",
    "sentences = [nltk.word_tokenize(sent) for sent in sentences] # NLTK word tokenizer\n",
    "sentences = [nltk.pos_tag(sent) for sent in sentences] # NLTK POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Patterns = \"\"\"\n",
    "        NP:     {<DT|PP\\$>?<JJ>*<NN>}\n",
    "                {<NNP>+}\n",
    "                {<NN>+}\n",
    "\"\"\"\n",
    "\n",
    "NPChunker = nltk.RegexpParser(patterns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/N-gram'\n",
    "html = urllib.request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "# kill all script and style elements\n",
    "for script in soup([\"script\", \"style\"]):\n",
    "    script.extract()    # rip it out\n",
    "\n",
    "# get text\n",
    "text = soup.get_text()\n",
    "\n",
    "# break into lines and remove leading and trailing space on each\n",
    "lines = (line.strip() for line in text.splitlines())\n",
    "# break multi-headlines into a line each\n",
    "chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "# drop blank lines\n",
    "text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "#textFiltered = filter(None,[word.strip(string.punctuation)\n",
    "#                 for word in text.replace(';','; ').split()\n",
    "#                 ])\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "finder = TrigramCollocationFinder.from_words(tokens)\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "scored = finder.score_ngrams(trigram_measures.raw_freq)\n",
    "sorted(finder.nbest(trigram_measures.raw_freq, 10))  # Return 10 trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a corpus and conduct analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Concatenate 'PublicRemarks' into a corpus\n",
    "def concatCol(col):\n",
    "    text = ''  # Declare text as string\n",
    "    for n in range(len(col)): # \n",
    "        text += str(col[n])+' '\n",
    "    return text\n",
    "\n",
    "corpus = concatCol(df['PublicRemarks'])\n",
    "\n",
    "## Save Text to File\n",
    "file = open('PublicRemarksCorpus.txt', \"w\")\n",
    "file.write(corpus)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read 'PublicRemarks' Text from file\n",
    "textfile = open('PublicRemarksCorpus.txt', 'r') # File\n",
    "corpus = textfile.read()  # String\n",
    "#corpus[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def get_ngrams(text, n ):\n",
    "    text = re.sub(r'[\\b\\(\\)\\\\\\\"\\'\\/\\[\\]\\s+\\,\\.:\\?;]', ' ', text)\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    n_grams = ngrams(tokens, n)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    "\n",
    "unigrams = get_ngrams(corpus, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
